---
title: "MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space"
collection: publications
status: "active"
permalink: /publication/2021-01-14-modals-1
excerpt: ''
date: 2021-01-14
venue: 'the International Conference of Learning Representation (ICLR 2021)'
paperurl: 'https://openreview.net/pdf?id=XjYgR6gbCEc'
code: 'https://github.com/jamestszhim/modals'
---

Data augmentation is an efficient way to expand a training dataset by creating additional artificial data. While data augmentation is found to be effective in improving the generalization capabilities of models for various machine learning tasks,
the underlying augmentation methods are usually manually designed and carefully evaluated for each data modality separately. These include image processing
functions for image data and word-replacing rules for text data. In this work, we
propose an automated data augmentation approach called MODALS (Modalityagnostic Automated Data Augmentation in the Latent Space) to augment data for
any modality in a generic way. MODALS exploits automated data augmentation
to fine-tune four universal data transformation operations in the latent space to
adapt the transform to data of different modalities. Through comprehensive experiments, we demonstrate the effectiveness of MODALS on multiple datasets for
text, tabular, time-series and image modalities.

[pdf](https://openreview.net/pdf?id=XjYgR6gbCEc)
[code](https://github.com/jamestszhim/modals)
